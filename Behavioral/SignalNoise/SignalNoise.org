#+STARTUP: beamer 
#+LATEX_CLASS: beamer
#+BEAMER_THEME: Montpellier
#+LaTeX_CLASS_OPTIONS: [bigger]
#+OPTIONS: H:3 toc:nil
#+toc: nil
#+TITLE: The Signal and the Noise
#+AUTHOR: Timothy Schwieg
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)

* Introduction 

*** Prediction

- Separate Signal from the Noise
- We're Mostly bad at this. Very Bad
- Information Overload  Printing Press
- Productivity Paradox  Information Age

* Prediction - Where it works and where it doesn't
** The Great Recession

*** Rating Agencies 
- Forecasted the risk of CDOs at 0.12\%
- Actual Default Rate at 28\%
- Rating Quality had little effect on their profits
- Benefited almost solely from rating securities
- Simulated under assumptions and missed risk they were taking


*** Risk and Uncertainty
- Depending on the Dependence structure of the underlying assets,
  bundled securities can be much less or more risky than under
  assumptions such as independence.
- The Nature of the dependence is not known - Knightian Uncertainty
- Rating agencies claimed this uncertainty was risk - Moody's
  increased their estimate of default by 50\%. Which did nothing to
  mollify.

*** Prediction in the Housing Bubble

- Homeowners assumed the prices would continue to rise -ignoring the
  bubble
- Banks and rating agencies did not understand the system risk present
- Economists did not understand how the leveraged bank sector was tied
  to the housing market
- Policy makers were overconfident in the rate of Recovery of the
  Economy - basing it falsely on recessions instead of financial
  crises.

*** Understanding Your Sample
- Should you Drunk Drive or Call Uber?
- Can we trust Rating Agencies to know how to rate new assets?
- You should always use your sample to avoid false confidence

** Political Predictions

*** McLaughlin Show
- Predictions occur at the final part of the episode
- Regardless of Political Affiliation, no guess is any better than 50%
- However the Predictions are structured with 75z% of them being
  completely false or Completely true

*** Political Scientists
- Almost nobody correctly predicted the Collapse of the Soviet Union
- Required piecing together structural problems with Soviet union and
  Gorbachev's motivations for reform.
- Poor estimates of Soviet GDP and its decline
- However this isn't unique to the Soviet Union - most political
  scientists are terrible at predicting anything.
- In fact, the ones that speak to the media do worse than ones that don't

*** Foxes vs Hedgehogs
- "The Fox knows many little things, but the hedgehog knows one big
  thing." - Archilochus
- Hedgehogs - Big ideas that govern all motion around us. Think Karl
  Marx
- Foxes - little ideas explain a little of an issue, but many of them
  combine to explain the picture. Usually multidisciplinary, cautious
  and empirical, as well as tolerant of complexity.
- Hedgehogs however make much better television guests, and receive
  the lion's share of publicity despite being worse at predictions.

*** Lessons from Hedgehogs
- Hedgehogs get worse at predicting as they get more experience
- They are letting their biases take over their prediction
- This occurs with television pundits and partisanship. 
- Don't construct narratives, these become less like reality the more
  information you get, and become worse predictors.

*** How to Apply this to Prediction
- Think Probabilistically ( Bayesian v Frequentist )
- Keep making forecasts and don't let the past affect today's prediction
- Consensus is a good sign, but not always
- There is no single data point that is sufficient for a complex
  system.
- Weigh Quantitative and Qualitative data together - Cook Political
  Report.

** Baseball

*** What makes baseball so easy to predict
- There is a lot of good data
- Play is largely independent. Players responsible for their own
  statistics
- Relatively few problems with complexity or dynamics
- We understand the processes underlying the data
- Easy to account for unobserved heterogeneity.

*** PECOTA isn't God's gift to scouting
- Outclassed by Professional scouting in predicting players' ability
  from minor to major league.
- A hybrid approach is the best predictive measure. 
- Especially since there is interdependence between certain aspects
  (Pitchers in minor leagues aren't playing against great hitters).
- Bias can happen in statistical models - Billy Beane had bad
  defenses.

*** Don't Limit the Data
- Being able to combine qualitative information is important for
  prediction
- Baseball scouts exist to find out this information and bring to
  attention
- Beware of discounting something we can't categorize easily such as
  the tool kit of Dustin Pedroia

** Weather Prediction - A Success Story

*** Predicting Weather
- Laplace's Demon - If you knew the exact state of the universe and
  all the laws of motion, we could predict perfectly.
- While this may be false, if we know the laws of motion and know the
  states of the universe, we're going to make pretty good predictions.
- Weather is something that we understand the underlying motion very well.
- We don't however know the state of every molecule in Earth's atmosphere

*** Predicting Weather
- We can however, discretize space and apply equations to data
  gathered. First attempted by Lewis Richardson
- However there is a lot of number crunching, so computers are pretty
  much required.
- However, as you become more precise in the discretization of the
  atmosphere, there is a computational cost. Doubling Precision
  increases the grid by 16 times as much.
- However this is a dynamic system, with one dimension being time, and
  we are not very good at predicting these.

*** Chaos Theory
- Applies to non-linear dynamic systems. 
- Tiny changes in the initial conditions can have huge consequences in
  the outcomes, even in a completely deterministic model.
- This means measurement error, and human error can have huge
  consequences in the predictions.
- This is handled by perturbation of the initial conditions and simulation.

*** The Human Touch
- It doesn't end there, there is still a human element to weather predictions. 
- Humans attempt to account for mistakes that the computer may make,
  through their intuition about the machinations of the model.
- This is true because humans are much better still at visually
  identifying patterns and can correct for it much better than a
  computer can. 
- This can lead to problems such as over fitting, but meteorologists
  improve predictions by about 25%, and predictions keep improving

*** Evaluating a Forecast
- Industry competition is not always seeking a more precise forecast,
  but a more useful one.
- The Economist's view on the Receiver Operating Curve.
- This can lead to dishonest predictions that are not necessarily bad,
  they simply are responding to other objectives.
- Be aware of goals other than strictly reporting truth.


* Prediction - Where it really doesn't work
** Earthquakes

*** Not such a success story - Earthquakes
- Officially "cannot be predicted" - Only Forecasted
- While we know the long-term rates at which earthquakes hit and the
  probabilities of earthquakes of different intensities, we can't
  really tell anything in the short-term. 
- No real understanding of the mechanics that are working "under the
  hood"
- Data is relatively poor on anything but relatively powerful quakes
  in many places.

*** Purely Statistical Models Don't Work
- While aftershocks may be able to be predicted, especially involving
  movement down fault lines, we would like to predict the first shock.
- Without theory, you will simply be finding patterns in the noise. 
- It is impossible to identify the signal in data that is this noisy
  without knowing what it is you are looking for.
- This is different from weather prediction, since we understand the
  laws of motion there.
- It is too easy to over fit because of the noise and large amount of data.

*** Why they aren't working (and may never)
- The Data is extremely noisy. 
- "With four parameters I can fit an elephant, with five I can make
  him wiggle his trunk" - Von Veuman
- Making a model more complex often makes it worse.
- Non parametric fits that diverge from theory can be from over fitting
  and missing rare events
- Complex Process - Many simple actors combine for a very complicated process

** Economics
  
*** Economics - Not so good with the predictions
- Economists are very bad at predicting, and often don't the
  uncertainty of their forecasts
- Often too overconfident - Despite there being readily available
  feedback
- Predictions fail to be accurate or precise
- "Nobody has a clue" - Jan Hatzius 

*** Why do the predictions fail so badly
- The underlying processes are both dynamic and complex
- Cause and effect are extremely hard to determine
- The Data is horrible
- The Lucas Critique
- Goodhart's law - Targetting an indicator removes its predictive
  power

*** Structural Changes
- Since the economy is a complex dynamic system, it is changing over
  time.
- In particular becoming a global economy has shifted capital levels
- Baby boomers retiring, declining middle class and increased debt
  have all altered the way the American Economy works.
- Much of the recent growth during The Great Moderation was
  debt-fueled

*** Can these problems be handled?
- Don't throw out data - Silver claims the fed overvalued data from
  the Great Moderation to determine their forecasts
- To not throw out data is to assume there is no structural change
  over time, must make an assumption one way or another.
- How do we know when the economy has changed structurally? 

*** Why have many of the Problems been resolved in weather and not economics?
- There is no hard science to turn back to. 
- The structural approach which attempts this still has to make many
  unrealistic assumptions
- The system is far more complex 
- Agents are all individually making decisions.

*** Computers don't help
- They just give you faster and bigger ways to mistake the signal for
  the noise.
- Models need to be about the economics going on behind the scenes,
  not the data that is produced by the economics.
- Stories about data are still just made by hedgehogs, and are
  unlikely to be anything but over fitting noise.

*** Incentives
- Forecasters who do not have their names attached to forecasts
  usually do better
- They aren't trying to make a name for themselves or trying to stay
  conservative to damage established reputation.
- Market for predictions or changing how we consume forecasts?

** The Flu
*** Predictions can influence behavior
- Predictions can undermine themselves - GPS diverts all traffic and
  creates a bigger jam
- Economic Predictors of recessions make actually cause them
- Flu predictions can increase vaccinations and reduce flu damage

*** A Solution to Complex Systems?
- When systems are complex because of actors - they can be simulated
- However these simulations need all the actions of the actors
  predicted, often by models that are unreliable.
- Worse they can come from psychology, or just be made up by the
  creators.
- For now have relatively little predictive power and are very hard to
  test

* Predicting Better
** Thinking Bayesian
*** False Positives
- Bayes theorem tells us that if the underlying probability of
  something is very low then false positives can dominate the results.
- Tests that are 95% accurate for rare diseases can only be 10-30%
  sure that you have the disease. 
- Research is especially affected by this because of the bias for
  results and the "publish or die" mentality.

*** Fisherian vs Bayesians
- Frequentists take a strong stance on the structure of the problem
- This eliminates the prior, but also eliminates sources of error
  besides the sampling error
- Requires a sample and many other assumptions that give it
  computational tractability at the cost of rigidity.
- Since the prior is eliminated it eliminates context which takes the
  form of a Bayesian Prior.

*** Gambling
- Bob Voulgaris - Bayesian approach to betting - everything is looked
  at in the context of probabilities.
- No notion of statistical significance, so there is no arbitrary
  cutoff levels at 95%.
- When rewards are occurring at the margin, as is the case in almost
  all competitive predictive markets, this is the approach that is
  needed.
- Hypothesis are still tested and examined in the context of some
  theory, but not tested in the Null hypothesis framework.

*** Less structure means more work
- Since there is much less structure placed on the model, our beliefs
  weigh stronger into the prediction.
- But no matter what our prior beliefs are, with enough data we should
  still see our beliefs converge.
- The literature is also on the path towards Bayesian approaches and
  is "converging" there.

** Computers - Where they are useful and where they are not

*** Understand the underlying processes
- Computers have drastically helped weather prediction, where we
  understand the fluid dynamics that explain their machinations.
- Relatively useless at predicting things in Economics and Earthquakes
  where we don't understand the underlying process.

*** Computers work better with good data
- Computers are very likely to mistake the signal for the noise
- They lack the sight to see the big picture, and compensate by brute
  force and look-ahead algorithms with pruning.
- However they are still victims of the assumptions that their
  programmers have put on them.

** Poker 

*** Poker
- Pareto Principle - Diminishing returns to skill in poker
- Very noisy output - randomness plays a large role in returns
- Feeding on overconfidence of the fish.

*** Tilting
- Tilting results from the fact that skill matters, but the game is
  still extremely noisy, and this can effect how we play.
- In all things avoid entitlement tilting - thinking that because you
  acted optimally, that you deserved to win.
- Don't be seduced by noisy positive results meaning that you are
  doing better
- Cannot evaluate predictions in noisy areas solely by right and
  wrong, have to evaluate the process.

** Group Forecasts

*** The Wisdom of Crowds
- Aggregation is incredibly powerful in certain situations
- However when people are reacting to other bets made, as is the case
  in a market, the dynamics are not as simple.
- The aggregate is only sometimes better than the best individual prediction.


* Vaguely Political Things

** Climate Science

*** Climate Prediction
- We understand the fundamental process that is heating the earth - The Greenhouse effect.
- However the primary cause for heating the earth isn't C0_2, its the
  water vapor stored in the air that becomes possible when the earth
  heats.
- While we understand the root cause very well, we are not very good
  at predicting temperatures.

*** Problems 
- Fancy models in the time from 1995-2012 were outperformed by a
  linear regression on C0_2 only.
- Complex systems that are influenced by lots of noise and cyclical
  trends like El Ni√±o rarely perform well.
- Scott Armstrong bet Al Gore in 2007 that temperatures would not
  increase each month over the next Decade and won.
- Large structural uncertainty and initial condition uncertainty. 
- From 2001 to 2011 temperatures declined (though not by much)

*** A Bayessian Explanation
- Based on some prior belief of temperatures increasing, we can look
  at how our beliefs in global warming can change with a decade of
  temperatures decreasing.
- The more certain you are in your prior, the more hurt you are
  going to get by being wrong.
- 95% sure of Global warming updates to 85%
- 99% sure of Global warming updates to 28%.
- Overconfidence in something can reduce public faith when you are
  incorrect.

** Unknown Unknowns

*** Uncertainty is back
- When we are unfamiliar with a possibility, we don't even think about
  it happening.
- "there are also unknown unknowns ... things we do not know we don't
  know" - Donald Rumsfeld
- It can be very hard to sort and build theories of what we should be
  looking for when there isn't a solid theory to lean back on
- Failures of predicting 9/11 and Pearl Harbor.

* Conclusions

** Conclusion

*** In Summary
- Try to understand the theory underlying a process
- Think probabilistically and Bayesian
- Stay simple, but don't make so many assumptions that you lose
  predictive power
- Keep your incentives on getting predictions right, and do not trust
  those without the same incentives.




